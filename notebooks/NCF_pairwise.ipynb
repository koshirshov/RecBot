{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2990ad8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T11:52:46.504300Z",
     "start_time": "2022-07-04T11:52:42.576295Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('/home/koshirshov/koshirshov/RecBot')\n",
    "import config\n",
    "from pandas_utils import load_data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e372fb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T11:52:53.384081Z",
     "start_time": "2022-07-04T11:52:49.250482Z"
    }
   },
   "outputs": [],
   "source": [
    "path = config.proceed_data_path\n",
    "users_df = load_data(path + 'users_processed.pkl',)\n",
    "items_df = load_data(path + 'items_processed.pkl',)\n",
    "train_interactions = load_data(path + 'train_interactions.pkl',)\n",
    "test_interactions = load_data(path + 'test_interactions.pkl',)\n",
    "sample_submission = load_data(path + 'sample_submission_processed.pkl',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64761ab2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T11:53:26.767788Z",
     "start_time": "2022-07-04T11:52:53.386712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Precision@1': 0.07489781772115592, 'Recall@1': 0.04826541257304038, 'Precision@3': 0.07616463895594809, 'Recall@3': 0.1375204993719853, 'Precision@10': 0.03765387097545237, 'Recall@10': 0.2108414786587126, 'MAP@10': 0.10311451737903853, 'Novelty@10': 3.435862548188481}\n"
     ]
    }
   ],
   "source": [
    "#popular\n",
    "from models import PopularRecommender\n",
    "from recommendation_utils import create_recommendations_from_one_model, create_submission_file\n",
    "from metrics import compute_metrics\n",
    "pop_model = PopularRecommender(days=30)\n",
    "pop_model.fit(train_interactions)\n",
    "df_recommendations = create_recommendations_from_one_model(pop_model, test_interactions['user_id'].unique())\n",
    "metrics = compute_metrics(train_interactions, test_interactions, df_recommendations)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e91236a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T11:53:26.788341Z",
     "start_time": "2022-07-04T11:53:26.770161Z"
    }
   },
   "outputs": [],
   "source": [
    "class Rating_Datset(torch.utils.data.Dataset):\n",
    "    def __init__(self, user_list, item_pos, item_neg):\n",
    "        super(Rating_Datset, self).__init__()\n",
    "        self.user_list = user_list\n",
    "        self.item_pos = item_pos\n",
    "        self.item_neg= item_neg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user = self.user_list[idx]\n",
    "        item_pos = self.item_pos[idx]\n",
    "        item_neg = self.item_neg[idx]\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(user, dtype=torch.long),\n",
    "            torch.tensor(item_pos, dtype=torch.long),\n",
    "            torch.tensor(item_neg, dtype=torch.long)\n",
    "            )\n",
    "    \n",
    "\n",
    "class NCF_Data(object):\n",
    "    \"\"\"\n",
    "    Construct Dataset for NCF\n",
    "    \"\"\"\n",
    "    def __init__(self, train_interactions, test_interactions, num_neg_train, num_neg_test, batch_size):\n",
    "        self.num_neg_train = num_neg_train\n",
    "        self.num_neg_test = num_neg_test\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.train_interactions, self.test_interactions = self._reindex(train_interactions, test_interactions)\n",
    "        self.user_pool = set(self.train_interactions['user_id'])\n",
    "        self.item_pool = set(self.train_interactions['item_id'])\n",
    "        \n",
    "    def _reindex(self, train_interactions, test_interactions):\n",
    "        \"\"\"\n",
    "        Process dataset to reindex userID and itemID, also set rating as binary feedback\n",
    "        \"\"\"\n",
    "        #ratings = ratings[ratings['watched_pct'] > 25]\n",
    "        all_users = set(train_interactions['user_id'])\n",
    "        train_interactions['quantity'] = 1\n",
    "        good_users = train_interactions.groupby('user_id')['quantity'].sum()\n",
    "        good_users = set(good_users[good_users >= 5].index)\n",
    "        good_items = train_interactions.groupby(['item_id'])['quantity'].sum()\n",
    "        good_items = set(good_items[good_items >= 10].index)\n",
    "        \n",
    "        train_interactions = train_interactions[train_interactions['user_id'].isin(good_users)]\n",
    "        train_interactions = train_interactions[train_interactions['item_id'].isin(good_items)]\n",
    "        \n",
    "        self.users_not_in_model = all_users - set(train_interactions['user_id'])\n",
    "        \n",
    "        user_list = list(train_interactions['user_id'].drop_duplicates())\n",
    "        self.user2id = {w: i for i, w in enumerate(user_list)}\n",
    "        self.user2id_reverse = {w: i for i, w in enumerate(user_list)}\n",
    "        \n",
    "        item_list = list(train_interactions['item_id'].drop_duplicates())\n",
    "        self.item2id = {w: i for i, w in enumerate(item_list)}\n",
    "        self.item2id_reverse = {i: w for i, w in enumerate(item_list)}\n",
    "        \n",
    "        self.users_for_prediction_model = (set(train_interactions['user_id'])&set(test_interactions['user_id'])) - self.users_not_in_model\n",
    "        self.users_for_prediction_popular = (set(test_interactions['user_id']) - self.users_for_prediction_model).union(self.users_not_in_model)\n",
    "        self.users_for_prediction_model = [user for user in self.users_for_prediction_model]\n",
    "        self.users_for_prediction_popular = [user for user in self.users_for_prediction_popular]\n",
    "        \n",
    "        train_interactions.loc[:,'user_id'] = train_interactions.loc[:,'user_id'].apply(lambda x: self.user2id[x])\n",
    "        train_interactions.loc[:,'item_id'] = train_interactions.loc[:,'item_id'].apply(lambda x: self.item2id[x])\n",
    "        train_interactions.loc[:,'rating'] = train_interactions.loc[:,'total_dur'].apply(lambda x: float(x >= 0))\n",
    "        train_interactions = train_interactions[['user_id', 'item_id', 'rating', 'date']]\n",
    "        \n",
    "        test_interactions['rating'] = 1\n",
    "        test_interactions = test_interactions[test_interactions['user_id'].isin(self.user2id.keys())]\n",
    "        test_interactions = test_interactions[test_interactions['item_id'].isin(self.item2id.keys())]\n",
    "        test_interactions = test_interactions.sample(30000)\n",
    "        test_interactions.loc[:,'user_id'] = test_interactions.loc[:,'user_id'].apply(lambda x: self.user2id[x])\n",
    "        test_interactions.loc[:,'item_id'] = test_interactions.loc[:,'item_id'].apply(lambda x: self.item2id[x])\n",
    "        return train_interactions[['user_id', 'item_id', 'rating']], test_interactions[['user_id', 'item_id', 'rating']]\n",
    "    \n",
    "    def _leave_one_out(self, ratings):\n",
    "        \"\"\"\n",
    "        leave-one-out evaluation protocol in paper https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf\n",
    "        \"\"\"\n",
    "        ratings['rank_latest'] = ratings.groupby(['user_id'])['date'].rank(method='first', ascending=False)\n",
    "        test = ratings.loc[ratings['rank_latest'] == 1]\n",
    "        train = ratings.loc[ratings['rank_latest'] > 1]\n",
    "        assert train['user_id'].nunique()==test['user_id'].nunique(), 'Not Match Train User with Test User'\n",
    "        return train[['user_id', 'item_id', 'rating']], test[['user_id', 'item_id', 'rating']]\n",
    "    \n",
    "    def _negative_sampling(self, interactions, n):\n",
    "        interact_status = (\n",
    "            interactions.groupby('user_id')['item_id']\n",
    "            .apply(set)\n",
    "            .reset_index()\n",
    "            .rename(columns={'item_id': 'interacted_items'}))\n",
    "        interact_status['negative_samples'] = interact_status.loc[:,'interacted_items'].apply(lambda x: random.sample(self.item_pool - x, n))\n",
    "        return interact_status[['user_id', 'negative_samples']]\n",
    "    \n",
    "    def get_train_instance(self):\n",
    "        kwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n",
    "        users, items_pos, items_neg = [], [], []\n",
    "        negatives_train = self._negative_sampling(self.train_interactions, self.num_neg_train)\n",
    "        train_interactions = pd.merge(self.train_interactions, negatives_train, on='user_id')\n",
    "        self.negatives_test = self._negative_sampling(test_interactions, self.num_neg_test)     \n",
    "        for row in train_interactions.itertuples():\n",
    "            for i in range(self.num_neg_train):\n",
    "                users.append(int(row.user_id))\n",
    "                items_pos.append(int(row.item_id))\n",
    "                items_neg.append(int(row.negative_samples[i]))\n",
    "                \n",
    "        dataset = Rating_Datset(\n",
    "            user_list=users,\n",
    "            item_pos=items_pos,\n",
    "            item_neg=items_neg)\n",
    "        \n",
    "        return torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True, **kwargs)\n",
    "    \n",
    "    def get_test_instance(self, ):\n",
    "        kwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n",
    "        users, items_pos, items_neg = [], [], []\n",
    "        negatives_test = self._negative_sampling(self.test_interactions, self.num_neg_test)\n",
    "        test_interactions = pd.merge(self.test_interactions, negatives_test, on='user_id')\n",
    "        \n",
    "        for row in test_interactions.itertuples():\n",
    "            for i in range(self.num_neg_train):\n",
    "                users.append(int(row.user_id))\n",
    "                items_pos.append(int(row.item_id))\n",
    "                items_neg.append(int(row.negative_samples[i]))\n",
    "                \n",
    "        dataset = Rating_Datset(\n",
    "            user_list=users,\n",
    "            item_pos=items_pos,\n",
    "            item_neg=items_neg)\n",
    "        \n",
    "        return torch.utils.data.DataLoader(dataset, batch_size=self.num_neg_test, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "315dab10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T11:54:44.588368Z",
     "start_time": "2022-07-04T11:53:26.790978Z"
    }
   },
   "outputs": [],
   "source": [
    "data = NCF_Data(train_interactions, test_interactions, num_neg_train=4, num_neg_test=25, batch_size=256)\n",
    "train_loader = data.get_train_instance()\n",
    "test_loader = data.get_test_instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876516bd",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## GMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb0a7c84",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Generalized_Matrix_Factorization(nn.Module):\n",
    "    def __init__(self, num_users, num_items, factor_num):\n",
    "        super(Generalized_Matrix_Factorization, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.factor_num = factor_num\n",
    "        \n",
    "        self.embedding_user = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num)\n",
    "        self.embedding_item = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num)\n",
    "\n",
    "        self.affine_output = nn.Linear(in_features=self.factor_num, out_features=1)\n",
    "        self.logistic = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embedding = self.embedding_user(user_indices)\n",
    "        item_embedding = self.embedding_item(item_indices)\n",
    "        element_product = torch.mul(user_embedding, item_embedding)\n",
    "        logits = self.affine_output(element_product)\n",
    "        rating = self.logistic(logits)\n",
    "        return rating\n",
    "\n",
    "    def init_weight(self):\n",
    "        nn.init.normal_(self.embedding_user.weight, std=0.01)\n",
    "        nn.init.normal_(self.embedding_item.weight, std=0.01)\n",
    "    \n",
    "    def recommend(self, users, batch_size, n = config.num_recommendations, show_progress_bar = False):\n",
    "        recs = list()\n",
    "        user_count = len(users)\n",
    "        \n",
    "        for start in tqdm(\n",
    "            range(0, user_count, batch_size),\n",
    "            desc=\"predict from model, users:\",\n",
    "            disable = not show_progress_bar\n",
    "        ):\n",
    "            end = start + batch_size\n",
    "            if end > user_count:\n",
    "                end = user_count\n",
    "                \n",
    "            user_indices = torch.tensor(users[start:end]).to(device).repeat_interleave(self.num_items)\n",
    "            item_indices = torch.tensor(range(self.num_items)).to(device).repeat((end-start))\n",
    "            predict = self.forward(user_indices, item_indices).view((end-start), -1)\n",
    "            predict = predict.topk(n)[1].detach().cpu()\n",
    "            \n",
    "            for rec in predict:\n",
    "                recs.append(rec.tolist())\n",
    "            \n",
    "        return recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fd49335",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Generalized_Matrix_Factorization(len(data.user2id), len(data.item2id), 64)\n",
    "model = model.to(device)\n",
    "model.init_weight()\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), 0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aa0c58d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalized_Matrix_Factorization(\n",
      "  (embedding_user): Embedding(281156, 64)\n",
      "  (embedding_item): Embedding(8250, 64)\n",
      "  (affine_output): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5032acc0",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 152657/152657 [19:27<00:00, 130.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 24\n",
      "The time for evaluate_model is: 00: 00: 52\n",
      "train_loss: 19035.51953125 \n",
      "test_loss: 34641.7578125 \n",
      "metrics_on_test_set: \n",
      "{'Precision@1': 0.06775103377393217, 'Recall@1': 0.04483039494926464, 'Precision@3': 0.06544645489239986, 'Recall@3': 0.12223095780492316, 'Precision@10': 0.03286624279943591, 'Recall@10': 0.18940929710410762, 'MAP@10': 0.09293149691786595, 'Novelty@10': 3.697623317192161}\n",
      "The time elapse of epoch 001 is: 00: 21: 52\n",
      "\n",
      "\n",
      "epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 152657/152657 [19:03<00:00, 133.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 21\n",
      "The time for evaluate_model is: 00: 00: 46\n",
      "train_loss: 22028.90625 \n",
      "test_loss: 19967.1640625 \n",
      "metrics_on_test_set: \n",
      "{'Precision@1': 0.07118698759471281, 'Recall@1': 0.04635837305881009, 'Precision@3': 0.0683047700998319, 'Recall@3': 0.12610507667865228, 'Precision@10': 0.034750340607596145, 'Recall@10': 0.19840607569504742, 'MAP@10': 0.09646168536512593, 'Novelty@10': 3.6179461263157755}\n",
      "The time elapse of epoch 002 is: 00: 21: 15\n",
      "\n",
      "\n",
      "epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 152657/152657 [41:33<00:00, 61.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 23\n",
      "The time for evaluate_model is: 00: 00: 48\n",
      "train_loss: 24736.955078125 \n",
      "test_loss: 15085.8173828125 \n",
      "metrics_on_test_set: \n",
      "{'Precision@1': 0.06993809307550733, 'Recall@1': 0.04587452857451676, 'Precision@3': 0.06787253706846412, 'Recall@3': 0.12547013849111388, 'Precision@10': 0.03490092501852427, 'Recall@10': 0.19846431906753123, 'MAP@10': 0.09598075133716916, 'Novelty@10': 3.594114421349478}\n",
      "The time elapse of epoch 003 is: 00: 43: 49\n",
      "\n",
      "\n",
      "epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 152657/152657 [19:15<00:00, 132.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 22\n",
      "The time for evaluate_model is: 00: 00: 48\n",
      "train_loss: 26978.453125 \n",
      "test_loss: 12048.2705078125 \n",
      "metrics_on_test_set: \n",
      "{'Precision@1': 0.07349953390539475, 'Recall@1': 0.0475177754415706, 'Precision@3': 0.07104357387001936, 'Recall@3': 0.12989589394069725, 'Precision@10': 0.03630817219207879, 'Recall@10': 0.20468962928054554, 'MAP@10': 0.09927905510724082, 'Novelty@10': 3.563676687238595}\n",
      "The time elapse of epoch 004 is: 00: 21: 29\n",
      "\n",
      "\n",
      "epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 152657/152657 [19:32<00:00, 130.21it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 66>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_history_train, loss_history_test, test_metrics_history\n\u001b[0;32m---> 66\u001b[0m loss_history_train, loss_history_test, test_metrics_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, loss_function, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     53\u001b[0m loss_accum \u001b[38;5;241m=\u001b[39m loss_accum\n\u001b[1;32m     54\u001b[0m loss_history_train\u001b[38;5;241m.\u001b[39mappend(loss_accum)\n\u001b[0;32m---> 55\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_test_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m loss_history_test\u001b[38;5;241m.\u001b[39mappend(test_loss)\n\u001b[1;32m     57\u001b[0m metrics \u001b[38;5;241m=\u001b[39m evaluate_model(model, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mcalculate_test_loss\u001b[0;34m(model, test_loader, loss_function)\u001b[0m\n\u001b[1;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (user, item, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_loader):\n\u001b[0;32m---> 23\u001b[0m     user \u001b[38;5;241m=\u001b[39m \u001b[43muser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     item \u001b[38;5;241m=\u001b[39m item\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m     label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, batch_size):\n",
    "    start_time = time.time()\n",
    "    model.eval()\n",
    "    preds = model.recommend([data.user2id[user_id] for user_id in data.users_for_prediction_model], batch_size = batch_size)\n",
    "    preds = [[data.item2id_reverse[item] for item in str_] for str_ in preds]\n",
    "    \n",
    "    df_recommendations = pd.DataFrame({'user_id': data.users_for_prediction_model})\n",
    "    df_recommendations['item_id'] = preds\n",
    "    df_recommendations = df_recommendations.explode('item_id')\n",
    "    df_recommendations['rank'] = df_recommendations.groupby('user_id').cumcount() + 1\n",
    "    \n",
    "    df_recommendations_popular = create_recommendations_from_one_model(pop_model, data.users_for_prediction_popular)\n",
    "    df_recommendations = pd.concat([df_recommendations, df_recommendations_popular], axis = 0)\n",
    "    metrics = compute_metrics(train_interactions, test_interactions, df_recommendations)\n",
    "    print(\"The time for evaluate_model is: \" + time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time)))\n",
    "    return metrics\n",
    "\n",
    "def calculate_test_loss(model, test_loader, loss_function):\n",
    "    start_time = time.time()\n",
    "    loss_accum = 0\n",
    "    model.eval()\n",
    "    for i, (user, item, label) in enumerate(test_loader):\n",
    "        user = user.to(device)\n",
    "        item = item.to(device)\n",
    "        label = label.to(device)\n",
    "        prediction = model(user, item).squeeze()\n",
    "        loss = loss_function(prediction, label)\n",
    "        loss_accum += loss\n",
    "    print(\"The time for test is: \" + time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time)))\n",
    "    return loss_accum\n",
    "\n",
    "def train_model(model, train_loader, test_loader, loss_function, optimizer, scheduler, num_epochs):\n",
    "    loss_history_train = []\n",
    "    loss_history_test = []\n",
    "    test_metrics_history = []\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"epoch: {epoch}\")\n",
    "        start_time = time.time()\n",
    "        loss_accum = 0\n",
    "        train_loader = data.get_train_instance() # refresh negative samples\n",
    "        model.train() # Enter train mode\n",
    "        for (user, item, label) in tqdm(train_loader):\n",
    "            user = user.to(device)\n",
    "            item = item.to(device)\n",
    "            label = label.to(device)  \n",
    "            prediction = model(user, item).squeeze()\n",
    "            loss = loss_function(prediction, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_accum += loss\n",
    "            \n",
    "        loss_accum = loss_accum\n",
    "        loss_history_train.append(loss_accum)\n",
    "        test_loss = calculate_test_loss(model, test_loader, loss_function)\n",
    "        loss_history_test.append(test_loss)\n",
    "        metrics = evaluate_model(model, batch_size=64)\n",
    "        test_metrics_history.append(metrics)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        print(f\"train_loss: {loss_accum} \\ntest_loss: {test_loss} \\nmetrics_on_test_set: \\n{metrics}\")\n",
    "        print(\"The time elapse of epoch {:03d}\".format(epoch) + \" is: \" + time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time)))\n",
    "        print(\"\\n\")\n",
    "    return loss_history_train, loss_history_test, test_metrics_history\n",
    "loss_history_train, loss_history_test, test_metrics_history = train_model(model, train_loader, test_loader, loss_function, optimizer, scheduler, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2150bb",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Multi_Layer_Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "479c7cc8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Multi_Layer_Perceptron(nn.Module):\n",
    "    def __init__(self, num_users, num_items, factor_num):\n",
    "        super(Multi_Layer_Perceptron, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.factor_num = factor_num\n",
    "        self.layers = [factor_num*2,32,16]\n",
    "\n",
    "        self.embedding_user = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num)\n",
    "        self.embedding_item = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num)\n",
    "\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        for idx, (in_size, out_size) in enumerate(zip(self.layers[:-1], self.layers[1:])):\n",
    "            self.fc_layers.append(nn.Linear(in_size, out_size))\n",
    "\n",
    "        self.affine_output = nn.Linear(in_features=self.layers[-1], out_features=1)\n",
    "        self.logistic = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embedding = self.embedding_user(user_indices)\n",
    "        item_embedding = self.embedding_item(item_indices)\n",
    "        vector = torch.cat([user_embedding, item_embedding], dim=-1)  # the concat latent vector\n",
    "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
    "            vector = self.fc_layers[idx](vector)\n",
    "            vector = nn.ReLU()(vector)\n",
    "            #vector = nn.BatchNorm1d()(vector)\n",
    "            vector = nn.Dropout(p=0.25)(vector)\n",
    "        logits = self.affine_output(vector)\n",
    "        rating = self.logistic(logits)\n",
    "        return rating\n",
    "\n",
    "    def init_weight(self):\n",
    "        nn.init.normal_(self.embedding_user.weight, std=0.01)\n",
    "        nn.init.normal_(self.embedding_item.weight, std=0.01)\n",
    "        \n",
    "    def recommend(self, users, batch_size, n = config.num_recommendations, show_progress_bar = False):\n",
    "        recs = list()\n",
    "        user_count = len(users)\n",
    "        \n",
    "        for start in tqdm(\n",
    "            range(0, user_count, batch_size),\n",
    "            desc=\"predict from model, users:\",\n",
    "            disable = not show_progress_bar\n",
    "        ):\n",
    "            end = start + batch_size\n",
    "            if end > user_count:\n",
    "                end = user_count\n",
    "                \n",
    "            user_indices = torch.tensor(users[start:end]).to(device).repeat_interleave(self.num_items)\n",
    "            item_indices = torch.tensor(range(self.num_items)).to(device).repeat((end-start))\n",
    "            predict = self.forward(user_indices, item_indices).view((end-start), -1)\n",
    "            predict = predict.topk(n)[1].detach().cpu()\n",
    "            \n",
    "            for rec in predict:\n",
    "                recs.append(rec.tolist())\n",
    "            \n",
    "        return recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc0b73a1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Multi_Layer_Perceptron(len(data.user2id), len(data.item2id), 64)\n",
    "model = model.to(device)\n",
    "model.init_weight()\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), 0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1347b4d5",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi_Layer_Perceptron(\n",
      "  (embedding_user): Embedding(281156, 64)\n",
      "  (embedding_item): Embedding(8250, 64)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=32, out_features=16, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9696b066",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 152657/152657 [21:15<00:00, 119.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 31\n",
      "The time for evaluate_model is: 00: 00: 52\n",
      "train_loss: 32706.845703125 \n",
      "test_loss: 94401.5546875 \n",
      "metrics_on_test_set: \n",
      "{'Precision@1': 0.06763749790854985, 'Recall@1': 0.044795864948753365, 'Precision@3': 0.06605197950777224, 'Recall@3': 0.12311731909516226, 'Precision@10': 0.03389344360255277, 'Recall@10': 0.19423844562481377, 'MAP@10': 0.09394437959415965, 'Novelty@10': 3.640449518629348}\n",
      "The time elapse of epoch 001 is: 00: 23: 47\n",
      "\n",
      "\n",
      "epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 152657/152657 [9:15:40<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 34\n",
      "The time for evaluate_model is: 00: 00: 56\n",
      "train_loss: 29794.197265625 \n",
      "test_loss: 134229.953125 \n",
      "metrics_on_test_set: \n",
      "{'Precision@1': 0.06633482324258431, 'Recall@1': 0.044171831389755126, 'Precision@3': 0.06483296284787789, 'Recall@3': 0.12144586626143039, 'Precision@10': 0.03340762961015369, 'Recall@10': 0.19193624566621187, 'MAP@10': 0.09269438637117772, 'Novelty@10': 3.673500529396491}\n",
      "The time elapse of epoch 002 is: 09: 18: 15\n",
      "\n",
      "\n",
      "epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|███████████████▍                                                                                                         | 19478/152657 [02:52<19:38, 112.98it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_history_train, loss_history_test, test_metrics_history\n\u001b[0;32m---> 68\u001b[0m loss_history_train, loss_history_test, test_metrics_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, loss_function, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     48\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     49\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 50\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     loss_accum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     53\u001b[0m loss_accum \u001b[38;5;241m=\u001b[39m loss_accum\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    155\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 157\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py:198\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madam\u001b[39m(params: List[Tensor],\n\u001b[1;32m    177\u001b[0m          grads: List[Tensor],\n\u001b[1;32m    178\u001b[0m          exp_avgs: List[Tensor],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m          eps: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    193\u001b[0m          maximize: \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Functional API that performs Adam algorithm computation.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    See :class:`~torch.optim.Adam` for details.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI has changed, `state_steps` argument must contain a list of singleton tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# Placeholder for more complex foreach logic to be added when value is not set\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, batch_size):\n",
    "    start_time = time.time()\n",
    "    model.eval()\n",
    "    preds = model.recommend([data.user2id[user_id] for user_id in data.users_for_prediction_model], batch_size = batch_size)\n",
    "    preds = [[data.item2id_reverse[item] for item in str_] for str_ in preds]\n",
    "    \n",
    "    df_recommendations = pd.DataFrame({'user_id': data.users_for_prediction_model})\n",
    "    df_recommendations['item_id'] = preds\n",
    "    df_recommendations = df_recommendations.explode('item_id')\n",
    "    df_recommendations['rank'] = df_recommendations.groupby('user_id').cumcount() + 1\n",
    "    \n",
    "    df_recommendations_popular = create_recommendations_from_one_model(pop_model, data.users_for_prediction_popular)\n",
    "    df_recommendations = pd.concat([df_recommendations, df_recommendations_popular], axis = 0)\n",
    "    metrics = compute_metrics(train_interactions, test_interactions, df_recommendations)\n",
    "    print(\"The time for evaluate_model is: \" + time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time)))\n",
    "    return metrics\n",
    "\n",
    "def calculate_test_loss(model, test_loader, loss_function):\n",
    "    start_time = time.time()\n",
    "    loss_accum = 0\n",
    "    model.eval()\n",
    "    for i, (user, item, label) in enumerate(test_loader):\n",
    "        user = user.to(device)\n",
    "        item = item.to(device)\n",
    "        label = label.to(device)\n",
    "        prediction = model(user, item).squeeze()\n",
    "        loss = loss_function(prediction, label)\n",
    "        loss_accum += loss\n",
    "    print(\"The time for test is: \" + time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time)))\n",
    "    return loss_accum\n",
    "\n",
    "def train_model(model, train_loader, test_loader, loss_function, optimizer, scheduler, num_epochs):\n",
    "    loss_history_train = []\n",
    "    loss_history_test = []\n",
    "    test_metrics_history = []\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"epoch: {epoch}\")\n",
    "        start_time = time.time()\n",
    "        loss_accum = 0\n",
    "        train_loader = data.get_train_instance() # refresh negative samples\n",
    "        model.train() # Enter train mode\n",
    "        for (user, item, label) in tqdm(train_loader):\n",
    "            user = user.to(device)\n",
    "            item = item.to(device)\n",
    "            label = label.to(device)  \n",
    "            prediction = model(user, item).squeeze()\n",
    "            loss = loss_function(prediction, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_accum += loss\n",
    "            \n",
    "        loss_accum = loss_accum\n",
    "        loss_history_train.append(loss_accum)\n",
    "        test_loss = calculate_test_loss(model, test_loader, loss_function)\n",
    "        loss_history_test.append(test_loss)\n",
    "        metrics = evaluate_model(model, batch_size=64)\n",
    "        test_metrics_history.append(metrics)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        print(f\"train_loss: {loss_accum} \\ntest_loss: {test_loss} \\nmetrics_on_test_set: \\n{metrics}\")\n",
    "        print(\"The time elapse of epoch {:03d}\".format(epoch) + \" is: \" + time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time)))\n",
    "        print(\"\\n\")\n",
    "    return loss_history_train, loss_history_test, test_metrics_history\n",
    "loss_history_train, loss_history_test, test_metrics_history = train_model(model, train_loader, test_loader, loss_function, optimizer, scheduler, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5453b2e3",
   "metadata": {},
   "source": [
    "## NeuMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5860dc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T11:54:44.599952Z",
     "start_time": "2022-07-04T11:54:44.589936Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, factor_num_mf, factor_num_mlp):\n",
    "        super(NeuMF, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.factor_num_mf = factor_num_mf\n",
    "        self.factor_num_mlp =  factor_num_mlp\n",
    "        self.layers = [2*factor_num_mlp, 32, 16]\n",
    "        self.dropout = 0.2\n",
    "        self.reg_1=0.0003\n",
    "        self.reg_2=0.0003\n",
    "        self.embedding_user_mlp = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_mlp)\n",
    "        self.embedding_item_mlp = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_mlp)\n",
    "\n",
    "        self.embedding_user_mf = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_mf)\n",
    "        self.embedding_item_mf = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_mf)\n",
    "\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        for idx, (in_size, out_size) in enumerate(zip(self.layers[:-1], self.layers[1:])):\n",
    "            self.fc_layers.append(nn.Dropout(p=self.dropout))\n",
    "            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
    "            self.fc_layers.append(nn.ReLU())\n",
    "\n",
    "        self.affine_output = nn.Linear(in_features=self.layers[-1] + self.factor_num_mf, out_features=1)\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        nn.init.normal_(self.embedding_user_mlp.weight, std=0.01)\n",
    "        nn.init.normal_(self.embedding_item_mlp.weight, std=0.01)\n",
    "        nn.init.normal_(self.embedding_user_mf.weight, std=0.01)\n",
    "        nn.init.normal_(self.embedding_item_mf.weight, std=0.01)\n",
    "        \n",
    "        for m in self.fc_layers:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                \n",
    "        nn.init.xavier_uniform_(self.affine_output.weight)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embedding_mlp = self.embedding_user_mlp(user_indices)\n",
    "        item_embedding_mlp = self.embedding_item_mlp(item_indices)\n",
    "\n",
    "        user_embedding_mf = self.embedding_user_mf(user_indices)\n",
    "        item_embedding_mf = self.embedding_item_mf(item_indices)\n",
    "\n",
    "        mlp_vector = torch.cat([user_embedding_mlp, item_embedding_mlp], dim=-1)  # the concat latent vector\n",
    "        mf_vector = torch.mul(user_embedding_mf, item_embedding_mf)\n",
    "\n",
    "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
    "            mlp_vector = self.fc_layers[idx](mlp_vector)\n",
    "\n",
    "        vector = torch.cat([mlp_vector, mf_vector], dim=-1)\n",
    "        logits = self.affine_output(vector)\n",
    "        return logits\n",
    "    \n",
    "    def recommend(self, users, batch_size, n = config.num_recommendations, show_progress_bar = False):\n",
    "        recs = list()\n",
    "        user_count = len(users)\n",
    "        \n",
    "        for start in tqdm(\n",
    "            range(0, user_count, batch_size),\n",
    "            desc=\"predict from model, users:\",\n",
    "            disable = not show_progress_bar\n",
    "        ):\n",
    "            end = start + batch_size\n",
    "            if end > user_count:\n",
    "                end = user_count\n",
    "                \n",
    "            user_indices = torch.tensor(users[start:end]).to(device).repeat_interleave(self.num_items)\n",
    "            item_indices = torch.tensor(range(self.num_items)).to(device).repeat((end-start))\n",
    "            predict = self.forward(user_indices, item_indices).view((end-start), -1)\n",
    "            predict = predict.topk(n)[1].detach().cpu()\n",
    "            \n",
    "            for rec in predict:\n",
    "                recs.append(rec.tolist())\n",
    "            \n",
    "        return recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e407002b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T11:54:48.683065Z",
     "start_time": "2022-07-04T11:54:44.602266Z"
    }
   },
   "outputs": [],
   "source": [
    "model = NeuMF(len(data.user2id), len(data.item2id), factor_num_mf=32, factor_num_mlp=32)\n",
    "model = model.to(device)\n",
    "model.init_weight()\n",
    "optimizer = optim.Adam(model.parameters(), 0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bd72aac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T11:54:48.687513Z",
     "start_time": "2022-07-04T11:54:48.684649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuMF(\n",
      "  (embedding_user_mlp): Embedding(281156, 32)\n",
      "  (embedding_item_mlp): Embedding(8250, 32)\n",
      "  (embedding_user_mf): Embedding(281156, 32)\n",
      "  (embedding_item_mf): Embedding(8250, 32)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (affine_output): Linear(in_features=48, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3665df9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T14:08:18.158767Z",
     "start_time": "2022-07-04T12:29:33.102633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 61063/61063 [13:04<00:00, 77.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 05\n",
      "The time for evaluate_model is: 00: 00: 51\n",
      "train_loss: 4253767.5 \n",
      "test_loss: 36232.01953125\n",
      "metrics_on_test_set: \n",
      "{'Precision@1': 0.07774218992757607, 'Recall@1': 0.04989046698821061, 'Precision@3': 0.07557106548430019, 'Recall@3': 0.13678386417309818, 'Precision@10': 0.03774828501087554, 'Recall@10': 0.21115707218306118, 'MAP@10': 0.10374922550530463, 'Novelty@10': 3.4559851433977307}\n",
      "The time elapse of epoch 001 is: 00: 15: 08\n",
      "\n",
      "\n",
      "epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 61063/61063 [13:01<00:00, 78.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 06\n",
      "The time for evaluate_model is: 00: 00: 52\n",
      "train_loss: 3618861.25 \n",
      "test_loss: 35490.10546875\n",
      "metrics_on_test_set: \n",
      "{'Precision@1': 0.07701914573224658, 'Recall@1': 0.04954284425073445, 'Precision@3': 0.07303941487200326, 'Recall@3': 0.13334000931490192, 'Precision@10': 0.03753077419509046, 'Recall@10': 0.210665921254339, 'MAP@10': 0.1028588590604676, 'Novelty@10': 3.4745896735383885}\n",
      "The time elapse of epoch 002 is: 00: 15: 04\n",
      "\n",
      "\n",
      "epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 61063/61063 [12:56<00:00, 78.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 06\n",
      "The time for evaluate_model is: 00: 00: 49\n",
      "train_loss: 3149312.75 \n",
      "test_loss: 37029.15625\n",
      "metrics_on_test_set: \n",
      "{'Precision@1': 0.07836962497310992, 'Recall@1': 0.049843168938969146, 'Precision@3': 0.07344376190134727, 'Recall@3': 0.1332448032446092, 'Precision@10': 0.037768601955207114, 'Recall@10': 0.21206831567820894, 'MAP@10': 0.10332783304768256, 'Novelty@10': 3.474939954039873}\n",
      "The time elapse of epoch 003 is: 00: 14: 55\n",
      "\n",
      "\n",
      "epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 61063/61063 [13:09<00:00, 77.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 07\n",
      "The time for evaluate_model is: 00: 02: 02\n",
      "train_loss: 2858805.5 \n",
      "test_loss: 37413.34375\n",
      "metrics_on_test_set: \n",
      "{'Precision@1': 0.07469464827784018, 'Recall@1': 0.04807109440385605, 'Precision@3': 0.07526431946203918, 'Recall@3': 0.1363453021167678, 'Precision@10': 0.03759112747089896, 'Recall@10': 0.2108726749249067, 'MAP@10': 0.10252421316046631, 'Novelty@10': 3.483877823511584}\n",
      "The time elapse of epoch 004 is: 00: 16: 21\n",
      "\n",
      "\n",
      "epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████████████████████████████████████████████▋                                                                            | 23159/61063 [36:04<59:02, 10.70it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 87>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_history_train, loss_history_test, test_metrics_history\n\u001b[0;32m---> 87\u001b[0m loss_history_train, loss_history_test, test_metrics_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, optimizer, scheduler, num_epochs, loss_type)\u001b[0m\n\u001b[1;32m     49\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget_train_instance() \u001b[38;5;66;03m# refresh negative samples\u001b[39;00m\n\u001b[1;32m     50\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m# Enter train mode\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (user, item_pos, item_neg) \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[1;32m     52\u001b[0m     user \u001b[38;5;241m=\u001b[39m user\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     53\u001b[0m     item_pos \u001b[38;5;241m=\u001b[39m item_pos\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    656\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    691\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    694\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mRating_Datset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     13\u001b[0m item_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_pos[idx]\n\u001b[1;32m     14\u001b[0m item_neg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_neg[idx]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m---> 17\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     18\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtensor(item_pos, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong),\n\u001b[1;32m     19\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtensor(item_neg, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     20\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, batch_size):\n",
    "    start_time = time.time()\n",
    "    model.eval()\n",
    "    preds = model.recommend([data.user2id[user_id] for user_id in data.users_for_prediction_model], batch_size = batch_size)\n",
    "    preds = [[data.item2id_reverse[item] for item in str_] for str_ in preds]\n",
    "    \n",
    "    df_recommendations = pd.DataFrame({'user_id': data.users_for_prediction_model})\n",
    "    df_recommendations['item_id'] = preds\n",
    "    df_recommendations = df_recommendations.explode('item_id')\n",
    "    df_recommendations['rank'] = df_recommendations.groupby('user_id').cumcount() + 1\n",
    "    \n",
    "    df_recommendations_popular = create_recommendations_from_one_model(pop_model, data.users_for_prediction_popular)\n",
    "    df_recommendations = pd.concat([df_recommendations, df_recommendations_popular], axis = 0)\n",
    "    metrics = compute_metrics(train_interactions, test_interactions, df_recommendations)\n",
    "    print(\"The time for evaluate_model is: \" + time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time)))\n",
    "    return metrics\n",
    "\n",
    "def calculate_test_loss(model, test_loader):\n",
    "    start_time = time.time()\n",
    "    loss_type = \"BPR\"\n",
    "    loss_accum = 0\n",
    "    model.eval()\n",
    "    for i, (user, item_pos, item_neg) in enumerate(test_loader):\n",
    "        user = user.to(device)\n",
    "        item_pos = item_pos.to(device)\n",
    "        item_neg = item_neg.to(device)\n",
    "        prediction_pos = model(user, item_pos).squeeze()\n",
    "        prediction_neg = model(user, item_neg).squeeze()\n",
    "        if loss_type == 'BPR':\n",
    "            loss = -((prediction_pos - prediction_neg).sigmoid() + 1e-24).log().sum()\n",
    "        elif loss_type == 'HL': #hinge_loss\n",
    "            loss = torch.clamp(1 - (prediction_pos - prediction_neg) * label, min=0).sum()\n",
    "        elif loss_type == 'TL':\n",
    "            loss = (prediction_neg - prediction_pos).sigmoid().mean() + prediction_neg.pow(2).sigmoid().mean()\n",
    "        loss_accum += loss\n",
    "    print(\"The time for test is: \" + time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time)))\n",
    "    return loss_accum\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, optimizer, scheduler, num_epochs, loss_type = \"BPR\"):\n",
    "    loss_history_train = []\n",
    "    loss_history_test = []\n",
    "    test_metrics_history = []\n",
    "    loss_type = \"BPR\"\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"epoch: {epoch}\")\n",
    "        start_time = time.time()\n",
    "        loss_accum = 0\n",
    "        train_loader = data.get_train_instance() # refresh negative samples\n",
    "        model.train() # Enter train mode\n",
    "        for (user, item_pos, item_neg) in tqdm(train_loader):\n",
    "            user = user.to(device)\n",
    "            item_pos = item_pos.to(device)\n",
    "            item_neg = item_neg.to(device)\n",
    "            prediction_pos = model(user, item_pos).squeeze()\n",
    "            prediction_neg = model(user, item_neg).squeeze()\n",
    "            if loss_type == 'BPR':\n",
    "                loss = -((prediction_pos - prediction_neg).sigmoid() + 1e-24).log().sum()\n",
    "            elif loss_type == 'HL':\n",
    "                loss = torch.clamp(1 - (prediction_pos - prediction_neg) * label, min=0).sum()\n",
    "            elif loss_type == 'TL':\n",
    "                loss = (prediction_neg - prediction_pos).sigmoid().mean() + prediction_neg.pow(2).sigmoid().mean()\n",
    "                \n",
    "            loss += model.reg_1 * (model.embedding_item_mf.weight.norm(p=1) + model.embedding_user_mf.weight.norm(p=1))\n",
    "            loss += model.reg_2 * (model.embedding_item_mlp.weight.norm(p=1) + model.embedding_user_mlp.weight.norm(p=1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_accum += loss\n",
    "            \n",
    "        loss_accum = loss_accum\n",
    "        loss_history_train.append(loss_accum)\n",
    "        test_loss = calculate_test_loss(model, test_loader)\n",
    "        loss_history_test.append(test_loss)\n",
    "        metrics = evaluate_model(model, batch_size=64)\n",
    "        test_metrics_history.append(metrics)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        print(f\"train_loss: {loss_accum} \\ntest_loss: {test_loss}\\nmetrics_on_test_set: \\n{metrics}\")\n",
    "        print(\"The time elapse of epoch {:03d}\".format(epoch) + \" is: \" + time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time)))\n",
    "        print(\"\\n\")\n",
    "    return loss_history_train, loss_history_test, test_metrics_history\n",
    "loss_history_train, loss_history_test, test_metrics_history = train_model(model, train_loader, test_loader, optimizer, scheduler, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
