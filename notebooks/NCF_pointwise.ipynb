{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2990ad8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T23:39:15.876814Z",
     "start_time": "2022-07-03T23:39:15.088475Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('/home/koshirshov/koshirshov/RecBot')\n",
    "import config\n",
    "from pandas_utils import load_data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e372fb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T23:39:16.749223Z",
     "start_time": "2022-07-03T23:39:15.882580Z"
    }
   },
   "outputs": [],
   "source": [
    "path = config.proceed_data_path\n",
    "users_df = load_data(path + 'users_processed.pkl',)\n",
    "items_df = load_data(path + 'items_processed.pkl',)\n",
    "train_interactions = load_data(path + 'train_interactions.pkl',)\n",
    "test_interactions = load_data(path + 'test_interactions.pkl',)\n",
    "sample_submission = load_data(path + 'sample_submission_processed.pkl',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64761ab2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T23:39:45.122137Z",
     "start_time": "2022-07-03T23:39:16.751323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Precision@1': 0.07489781772115592, 'Recall@1': 0.04826541257304038, 'Precision@3': 0.07616463895594809, 'Recall@3': 0.1375204993719853, 'Precision@10': 0.03765387097545237, 'Recall@10': 0.2108414786587126, 'MAP@10': 0.10311451737903853, 'Novelty@10': 3.435862548188481}\n"
     ]
    }
   ],
   "source": [
    "#popular\n",
    "from models import PopularRecommender\n",
    "from recommendation_utils import create_recommendations_from_one_model, create_submission_file\n",
    "from metrics import compute_metrics\n",
    "pop_model = PopularRecommender(days=30)\n",
    "pop_model.fit(train_interactions)\n",
    "df_recommendations = create_recommendations_from_one_model(pop_model, test_interactions['user_id'].unique())\n",
    "metrics = compute_metrics(train_interactions, test_interactions, df_recommendations)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e91236a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T23:39:45.138953Z",
     "start_time": "2022-07-03T23:39:45.123771Z"
    }
   },
   "outputs": [],
   "source": [
    "class Rating_Datset(torch.utils.data.Dataset):\n",
    "    def __init__(self, user_list, item_list, rating_list):\n",
    "        super(Rating_Datset, self).__init__()\n",
    "        self.user_list = user_list\n",
    "        self.item_list = item_list\n",
    "        self.rating_list = rating_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user = self.user_list[idx]\n",
    "        item = self.item_list[idx]\n",
    "        rating = self.rating_list[idx]\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(user, dtype=torch.long),\n",
    "            torch.tensor(item, dtype=torch.long),\n",
    "            torch.tensor(rating, dtype=torch.float)\n",
    "            )\n",
    "    \n",
    "\n",
    "class NCF_Data(object):\n",
    "    \"\"\"\n",
    "    Construct Dataset for NCF\n",
    "    \"\"\"\n",
    "    def __init__(self, train_interactions, test_interactions, num_neg_train, num_neg_test, batch_size):\n",
    "        self.num_neg_train = num_neg_train\n",
    "        self.num_neg_test = num_neg_test\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.train_interactions, self.test_interactions = self._reindex(train_interactions, test_interactions)\n",
    "        self.user_pool = set(self.train_interactions['user_id'])\n",
    "        self.item_pool = set(self.train_interactions['item_id'])\n",
    "        \n",
    "    def _reindex(self, train_interactions, test_interactions):\n",
    "        \"\"\"\n",
    "        Process dataset to reindex userID and itemID, also set rating as binary feedback\n",
    "        \"\"\"\n",
    "        #ratings = ratings[ratings['watched_pct'] > 25]\n",
    "        all_users = set(train_interactions['user_id'])\n",
    "        train_interactions['quantity'] = 1\n",
    "        good_users = train_interactions.groupby('user_id')['quantity'].sum()\n",
    "        good_users = set(good_users[good_users >= 5].index)\n",
    "        good_items = train_interactions.groupby(['item_id'])['quantity'].sum()\n",
    "        good_items = set(good_items[good_items >= 10].index)\n",
    "        \n",
    "        train_interactions = train_interactions[train_interactions['user_id'].isin(good_users)]\n",
    "        train_interactions = train_interactions[train_interactions['item_id'].isin(good_items)]\n",
    "        \n",
    "        self.users_not_in_model = all_users - set(train_interactions['user_id'])\n",
    "        \n",
    "        user_list = list(train_interactions['user_id'].drop_duplicates())\n",
    "        self.user2id = {w: i for i, w in enumerate(user_list)}\n",
    "        self.user2id_reverse = {w: i for i, w in enumerate(user_list)}\n",
    "        \n",
    "        item_list = list(train_interactions['item_id'].drop_duplicates())\n",
    "        self.item2id = {w: i for i, w in enumerate(item_list)}\n",
    "        self.item2id_reverse = {i: w for i, w in enumerate(item_list)}\n",
    "        \n",
    "        self.users_for_prediction_model = (set(train_interactions['user_id'])&set(test_interactions['user_id'])) - self.users_not_in_model\n",
    "        self.users_for_prediction_popular = (set(test_interactions['user_id']) - self.users_for_prediction_model).union(self.users_not_in_model)\n",
    "        self.users_for_prediction_model = [user for user in self.users_for_prediction_model]\n",
    "        self.users_for_prediction_popular = [user for user in self.users_for_prediction_popular]\n",
    "        \n",
    "        train_interactions.loc[:,'user_id'] = train_interactions.loc[:,'user_id'].apply(lambda x: self.user2id[x])\n",
    "        train_interactions.loc[:,'item_id'] = train_interactions.loc[:,'item_id'].apply(lambda x: self.item2id[x])\n",
    "        train_interactions.loc[:,'rating'] = train_interactions.loc[:,'total_dur'].apply(lambda x: float(x >= 0))\n",
    "        train_interactions = train_interactions[['user_id', 'item_id', 'rating', 'date']]\n",
    "        \n",
    "        test_interactions['rating'] = 1\n",
    "        test_interactions = test_interactions[test_interactions['user_id'].isin(self.user2id.keys())]\n",
    "        test_interactions = test_interactions[test_interactions['item_id'].isin(self.item2id.keys())]\n",
    "        test_interactions = test_interactions.sample(30000)\n",
    "        test_interactions.loc[:,'user_id'] = test_interactions.loc[:,'user_id'].apply(lambda x: self.user2id[x])\n",
    "        test_interactions.loc[:,'item_id'] = test_interactions.loc[:,'item_id'].apply(lambda x: self.item2id[x])\n",
    "        return train_interactions[['user_id', 'item_id', 'rating']], test_interactions[['user_id', 'item_id', 'rating']]\n",
    "    \n",
    "    def _leave_one_out(self, ratings):\n",
    "        \"\"\"\n",
    "        leave-one-out evaluation protocol in paper https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf\n",
    "        \"\"\"\n",
    "        ratings['rank_latest'] = ratings.groupby(['user_id'])['date'].rank(method='first', ascending=False)\n",
    "        test = ratings.loc[ratings['rank_latest'] == 1]\n",
    "        train = ratings.loc[ratings['rank_latest'] > 1]\n",
    "        assert train['user_id'].nunique()==test['user_id'].nunique(), 'Not Match Train User with Test User'\n",
    "        return train[['user_id', 'item_id', 'rating']], test[['user_id', 'item_id', 'rating']]\n",
    "    \n",
    "    def _negative_sampling(self, interactions, n):\n",
    "        interact_status = (\n",
    "            interactions.groupby('user_id')['item_id']\n",
    "            .apply(set)\n",
    "            .reset_index()\n",
    "            .rename(columns={'item_id': 'interacted_items'}))\n",
    "        interact_status['negative_samples'] = interact_status.loc[:,'interacted_items'].apply(lambda x: random.sample(self.item_pool - x, n))\n",
    "        return interact_status[['user_id', 'negative_samples']]\n",
    "    \n",
    "    def get_train_instance(self):\n",
    "        kwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n",
    "        users, items, ratings = [], [], []\n",
    "        negatives_train = self._negative_sampling(self.train_interactions, self.num_neg_train)\n",
    "        train_interactions = pd.merge(self.train_interactions, negatives_train, on='user_id')\n",
    "        self.negatives_test = self._negative_sampling(test_interactions, self.num_neg_test)     \n",
    "        for row in train_interactions.itertuples():\n",
    "            users.append(int(row.user_id))\n",
    "            items.append(int(row.item_id))\n",
    "            ratings.append(float(row.rating))\n",
    "            for i in range(self.num_neg_train):\n",
    "                users.append(int(row.user_id))\n",
    "                items.append(int(row.negative_samples[i]))\n",
    "                ratings.append(float(0))  # negative samples get 0 rating\n",
    "                \n",
    "        dataset = Rating_Datset(\n",
    "            user_list=users,\n",
    "            item_list=items,\n",
    "            rating_list=ratings)\n",
    "        \n",
    "        return torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True, **kwargs)\n",
    "    \n",
    "    def get_test_instance(self, ):\n",
    "        kwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n",
    "        users, items, ratings = [], [], []\n",
    "        negatives_test = self._negative_sampling(self.test_interactions, self.num_neg_test)\n",
    "        test_interactions = pd.merge(self.test_interactions, negatives_test, on='user_id')\n",
    "        \n",
    "        for row in test_interactions.itertuples():\n",
    "            users.append(int(row.user_id))\n",
    "            items.append(int(row.item_id))\n",
    "            ratings.append(float(row.rating))\n",
    "            for i in range(self.num_neg_test):\n",
    "                users.append(int(row.user_id))\n",
    "                items.append(int(row.negative_samples[i]))\n",
    "                ratings.append(float(0))  # negative samples get 0 rating\n",
    "                \n",
    "        dataset = Rating_Datset(\n",
    "            user_list=users,\n",
    "            item_list=items,\n",
    "            rating_list=ratings)\n",
    "        \n",
    "        return torch.utils.data.DataLoader(dataset, batch_size=self.num_neg_test + 1, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "315dab10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T23:41:00.470608Z",
     "start_time": "2022-07-03T23:39:45.141987Z"
    }
   },
   "outputs": [],
   "source": [
    "data = NCF_Data(train_interactions, test_interactions, num_neg_train=4, num_neg_test=25, batch_size=256)\n",
    "train_loader = data.get_train_instance()\n",
    "test_loader = data.get_test_instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876516bd",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## GMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb0a7c84",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Generalized_Matrix_Factorization(nn.Module):\n",
    "    def __init__(self, num_users, num_items, factor_num):\n",
    "        super(Generalized_Matrix_Factorization, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.factor_num = factor_num\n",
    "        \n",
    "        self.embedding_user = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num)\n",
    "        self.embedding_item = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num)\n",
    "\n",
    "        self.affine_output = nn.Linear(in_features=self.factor_num, out_features=1)\n",
    "        self.logistic = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embedding = self.embedding_user(user_indices)\n",
    "        item_embedding = self.embedding_item(item_indices)\n",
    "        element_product = torch.mul(user_embedding, item_embedding)\n",
    "        logits = self.affine_output(element_product)\n",
    "        rating = self.logistic(logits)\n",
    "        return rating\n",
    "\n",
    "    def init_weight(self):\n",
    "        nn.init.normal_(self.embedding_user.weight, std=0.01)\n",
    "        nn.init.normal_(self.embedding_item.weight, std=0.01)\n",
    "    \n",
    "    def recommend(self, users, batch_size, n = config.num_recommendations, show_progress_bar = False):\n",
    "        recs = list()\n",
    "        user_count = len(users)\n",
    "        \n",
    "        for start in tqdm(\n",
    "            range(0, user_count, batch_size),\n",
    "            desc=\"predict from model, users:\",\n",
    "            disable = not show_progress_bar\n",
    "        ):\n",
    "            end = start + batch_size\n",
    "            if end > user_count:\n",
    "                end = user_count\n",
    "                \n",
    "            user_indices = torch.tensor(users[start:end]).to(device).repeat_interleave(self.num_items)\n",
    "            item_indices = torch.tensor(range(self.num_items)).to(device).repeat((end-start))\n",
    "            predict = self.forward(user_indices, item_indices).view((end-start), -1)\n",
    "            predict = predict.topk(n)[1].detach().cpu()\n",
    "            \n",
    "            for rec in predict:\n",
    "                recs.append(rec.tolist())\n",
    "            \n",
    "        return recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fd49335",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Generalized_Matrix_Factorization(len(data.user2id), len(data.item2id), 64)\n",
    "model = model.to(device)\n",
    "model.init_weight()\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), 0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aa0c58d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalized_Matrix_Factorization(\n",
      "  (embedding_user): Embedding(281156, 64)\n",
      "  (embedding_item): Embedding(8250, 64)\n",
      "  (affine_output): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5032acc0",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 152657/152657 [19:27<00:00, 130.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 24\n",
      "The time for evaluate_model is: 00: 00: 52\n",
      "train_loss: 19035.51953125 \n",
      "test_loss: 34641.7578125 \n",
      "metrics_on_test_set: \n",
      "{'Precision@1': 0.06775103377393217, 'Recall@1': 0.04483039494926464, 'Precision@3': 0.06544645489239986, 'Recall@3': 0.12223095780492316, 'Precision@10': 0.03286624279943591, 'Recall@10': 0.18940929710410762, 'MAP@10': 0.09293149691786595, 'Novelty@10': 3.697623317192161}\n",
      "The time elapse of epoch 001 is: 00: 21: 52\n",
      "\n",
      "\n",
      "epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 152657/152657 [19:03<00:00, 133.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 21\n",
      "The time for evaluate_model is: 00: 00: 46\n",
      "train_loss: 22028.90625 \n",
      "test_loss: 19967.1640625 \n",
      "metrics_on_test_set: \n",
      "{'Precision@1': 0.07118698759471281, 'Recall@1': 0.04635837305881009, 'Precision@3': 0.0683047700998319, 'Recall@3': 0.12610507667865228, 'Precision@10': 0.034750340607596145, 'Recall@10': 0.19840607569504742, 'MAP@10': 0.09646168536512593, 'Novelty@10': 3.6179461263157755}\n",
      "The time elapse of epoch 002 is: 00: 21: 15\n",
      "\n",
      "\n",
      "epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 152657/152657 [41:33<00:00, 61.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 23\n",
      "The time for evaluate_model is: 00: 00: 48\n",
      "train_loss: 24736.955078125 \n",
      "test_loss: 15085.8173828125 \n",
      "metrics_on_test_set: \n",
      "{'Precision@1': 0.06993809307550733, 'Recall@1': 0.04587452857451676, 'Precision@3': 0.06787253706846412, 'Recall@3': 0.12547013849111388, 'Precision@10': 0.03490092501852427, 'Recall@10': 0.19846431906753123, 'MAP@10': 0.09598075133716916, 'Novelty@10': 3.594114421349478}\n",
      "The time elapse of epoch 003 is: 00: 43: 49\n",
      "\n",
      "\n",
      "epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 152657/152657 [19:15<00:00, 132.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 22\n",
      "The time for evaluate_model is: 00: 00: 48\n",
      "train_loss: 26978.453125 \n",
      "test_loss: 12048.2705078125 \n",
      "metrics_on_test_set: \n",
      "{'Precision@1': 0.07349953390539475, 'Recall@1': 0.0475177754415706, 'Precision@3': 0.07104357387001936, 'Recall@3': 0.12989589394069725, 'Precision@10': 0.03630817219207879, 'Recall@10': 0.20468962928054554, 'MAP@10': 0.09927905510724082, 'Novelty@10': 3.563676687238595}\n",
      "The time elapse of epoch 004 is: 00: 21: 29\n",
      "\n",
      "\n",
      "epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 152657/152657 [19:32<00:00, 130.21it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 66>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_history_train, loss_history_test, test_metrics_history\n\u001b[0;32m---> 66\u001b[0m loss_history_train, loss_history_test, test_metrics_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, loss_function, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     53\u001b[0m loss_accum \u001b[38;5;241m=\u001b[39m loss_accum\n\u001b[1;32m     54\u001b[0m loss_history_train\u001b[38;5;241m.\u001b[39mappend(loss_accum)\n\u001b[0;32m---> 55\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_test_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m loss_history_test\u001b[38;5;241m.\u001b[39mappend(test_loss)\n\u001b[1;32m     57\u001b[0m metrics \u001b[38;5;241m=\u001b[39m evaluate_model(model, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mcalculate_test_loss\u001b[0;34m(model, test_loader, loss_function)\u001b[0m\n\u001b[1;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (user, item, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_loader):\n\u001b[0;32m---> 23\u001b[0m     user \u001b[38;5;241m=\u001b[39m \u001b[43muser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     item \u001b[38;5;241m=\u001b[39m item\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m     label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, batch_size):\n",
    "    start_time = time.time()\n",
    "    model.eval()\n",
    "    preds = model.recommend([data.user2id[user_id] for user_id in data.users_for_prediction_model], batch_size = batch_size)\n",
    "    preds = [[data.item2id_reverse[item] for item in str_] for str_ in preds]\n",
    "    \n",
    "    df_recommendations = pd.DataFrame({'user_id': data.users_for_prediction_model})\n",
    "    df_recommendations['item_id'] = preds\n",
    "    df_recommendations = df_recommendations.explode('item_id')\n",
    "    df_recommendations['rank'] = df_recommendations.groupby('user_id').cumcount() + 1\n",
    "    \n",
    "    df_recommendations_popular = create_recommendations_from_one_model(pop_model, data.users_for_prediction_popular)\n",
    "    df_recommendations = pd.concat([df_recommendations, df_recommendations_popular], axis = 0)\n",
    "    metrics = compute_metrics(train_interactions, test_interactions, df_recommendations)\n",
    "    print(\"The time for evaluate_model is: \" + time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time)))\n",
    "    return metrics\n",
    "\n",
    "def calculate_test_loss(model, test_loader, loss_function):\n",
    "    start_time = time.time()\n",
    "    loss_accum = 0\n",
    "    model.eval()\n",
    "    for i, (user, item, label) in enumerate(test_loader):\n",
    "        user = user.to(device)\n",
    "        item = item.to(device)\n",
    "        label = label.to(device)\n",
    "        prediction = model(user, item).squeeze()\n",
    "        loss = loss_function(prediction, label)\n",
    "        loss_accum += loss\n",
    "    print(\"The time for test is: \" + time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time)))\n",
    "    return loss_accum\n",
    "\n",
    "def train_model(model, train_loader, test_loader, loss_function, optimizer, scheduler, num_epochs):\n",
    "    loss_history_train = []\n",
    "    loss_history_test = []\n",
    "    test_metrics_history = []\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"epoch: {epoch}\")\n",
    "        start_time = time.time()\n",
    "        loss_accum = 0\n",
    "        train_loader = data.get_train_instance() # refresh negative samples\n",
    "        model.train() # Enter train mode\n",
    "        for (user, item, label) in tqdm(train_loader):\n",
    "            user = user.to(device)\n",
    "            item = item.to(device)\n",
    "            label = label.to(device)  \n",
    "            prediction = model(user, item).squeeze()\n",
    "            loss = loss_function(prediction, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_accum += loss\n",
    "            \n",
    "        loss_accum = loss_accum\n",
    "        loss_history_train.append(loss_accum)\n",
    "        test_loss = calculate_test_loss(model, test_loader, loss_function)\n",
    "        loss_history_test.append(test_loss)\n",
    "        metrics = evaluate_model(model, batch_size=64)\n",
    "        test_metrics_history.append(metrics)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        print(f\"train_loss: {loss_accum} \\ntest_loss: {test_loss} \\nmetrics_on_test_set: \\n{metrics}\")\n",
    "        print(\"The time elapse of epoch {:03d}\".format(epoch) + \" is: \" + time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time)))\n",
    "        print(\"\\n\")\n",
    "    return loss_history_train, loss_history_test, test_metrics_history\n",
    "loss_history_train, loss_history_test, test_metrics_history = train_model(model, train_loader, test_loader, loss_function, optimizer, scheduler, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2150bb",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Multi_Layer_Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "479c7cc8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Multi_Layer_Perceptron(nn.Module):\n",
    "    def __init__(self, num_users, num_items, factor_num):\n",
    "        super(Multi_Layer_Perceptron, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.factor_num = factor_num\n",
    "        self.layers = [factor_num*2,32,16]\n",
    "\n",
    "        self.embedding_user = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num)\n",
    "        self.embedding_item = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num)\n",
    "\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        for idx, (in_size, out_size) in enumerate(zip(self.layers[:-1], self.layers[1:])):\n",
    "            self.fc_layers.append(nn.Linear(in_size, out_size))\n",
    "\n",
    "        self.affine_output = nn.Linear(in_features=self.layers[-1], out_features=1)\n",
    "        self.logistic = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embedding = self.embedding_user(user_indices)\n",
    "        item_embedding = self.embedding_item(item_indices)\n",
    "        vector = torch.cat([user_embedding, item_embedding], dim=-1)  # the concat latent vector\n",
    "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
    "            vector = self.fc_layers[idx](vector)\n",
    "            vector = nn.ReLU()(vector)\n",
    "            #vector = nn.BatchNorm1d()(vector)\n",
    "            vector = nn.Dropout(p=0.25)(vector)\n",
    "        logits = self.affine_output(vector)\n",
    "        rating = self.logistic(logits)\n",
    "        return rating\n",
    "\n",
    "    def init_weight(self):\n",
    "        nn.init.normal_(self.embedding_user.weight, std=0.01)\n",
    "        nn.init.normal_(self.embedding_item.weight, std=0.01)\n",
    "        \n",
    "    def recommend(self, users, batch_size, n = config.num_recommendations, show_progress_bar = False):\n",
    "        recs = list()\n",
    "        user_count = len(users)\n",
    "        \n",
    "        for start in tqdm(\n",
    "            range(0, user_count, batch_size),\n",
    "            desc=\"predict from model, users:\",\n",
    "            disable = not show_progress_bar\n",
    "        ):\n",
    "            end = start + batch_size\n",
    "            if end > user_count:\n",
    "                end = user_count\n",
    "                \n",
    "            user_indices = torch.tensor(users[start:end]).to(device).repeat_interleave(self.num_items)\n",
    "            item_indices = torch.tensor(range(self.num_items)).to(device).repeat((end-start))\n",
    "            predict = self.forward(user_indices, item_indices).view((end-start), -1)\n",
    "            predict = predict.topk(n)[1].detach().cpu()\n",
    "            \n",
    "            for rec in predict:\n",
    "                recs.append(rec.tolist())\n",
    "            \n",
    "        return recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc0b73a1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Multi_Layer_Perceptron(len(data.user2id), len(data.item2id), 64)\n",
    "model = model.to(device)\n",
    "model.init_weight()\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), 0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1347b4d5",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi_Layer_Perceptron(\n",
      "  (embedding_user): Embedding(281156, 64)\n",
      "  (embedding_item): Embedding(8250, 64)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (1): Linear(in_features=32, out_features=16, bias=True)\n",
      "  )\n",
      "  (affine_output): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9696b066",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 152657/152657 [21:15<00:00, 119.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 31\n",
      "The time for evaluate_model is: 00: 00: 52\n",
      "train_loss: 32706.845703125 \n",
      "test_loss: 94401.5546875 \n",
      "metrics_on_test_set: \n",
      "{'Precision@1': 0.06763749790854985, 'Recall@1': 0.044795864948753365, 'Precision@3': 0.06605197950777224, 'Recall@3': 0.12311731909516226, 'Precision@10': 0.03389344360255277, 'Recall@10': 0.19423844562481377, 'MAP@10': 0.09394437959415965, 'Novelty@10': 3.640449518629348}\n",
      "The time elapse of epoch 001 is: 00: 23: 47\n",
      "\n",
      "\n",
      "epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 152657/152657 [9:15:40<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 34\n",
      "The time for evaluate_model is: 00: 00: 56\n",
      "train_loss: 29794.197265625 \n",
      "test_loss: 134229.953125 \n",
      "metrics_on_test_set: \n",
      "{'Precision@1': 0.06633482324258431, 'Recall@1': 0.044171831389755126, 'Precision@3': 0.06483296284787789, 'Recall@3': 0.12144586626143039, 'Precision@10': 0.03340762961015369, 'Recall@10': 0.19193624566621187, 'MAP@10': 0.09269438637117772, 'Novelty@10': 3.673500529396491}\n",
      "The time elapse of epoch 002 is: 09: 18: 15\n",
      "\n",
      "\n",
      "epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|███████████████▍                                                                                                         | 19478/152657 [02:52<19:38, 112.98it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_history_train, loss_history_test, test_metrics_history\n\u001b[0;32m---> 68\u001b[0m loss_history_train, loss_history_test, test_metrics_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, loss_function, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     48\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     49\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 50\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     loss_accum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     53\u001b[0m loss_accum \u001b[38;5;241m=\u001b[39m loss_accum\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    155\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 157\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py:198\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madam\u001b[39m(params: List[Tensor],\n\u001b[1;32m    177\u001b[0m          grads: List[Tensor],\n\u001b[1;32m    178\u001b[0m          exp_avgs: List[Tensor],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m          eps: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    193\u001b[0m          maximize: \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Functional API that performs Adam algorithm computation.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    See :class:`~torch.optim.Adam` for details.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI has changed, `state_steps` argument must contain a list of singleton tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# Placeholder for more complex foreach logic to be added when value is not set\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, batch_size):\n",
    "    start_time = time.time()\n",
    "    model.eval()\n",
    "    preds = model.recommend([data.user2id[user_id] for user_id in data.users_for_prediction_model], batch_size = batch_size)\n",
    "    preds = [[data.item2id_reverse[item] for item in str_] for str_ in preds]\n",
    "    \n",
    "    df_recommendations = pd.DataFrame({'user_id': data.users_for_prediction_model})\n",
    "    df_recommendations['item_id'] = preds\n",
    "    df_recommendations = df_recommendations.explode('item_id')\n",
    "    df_recommendations['rank'] = df_recommendations.groupby('user_id').cumcount() + 1\n",
    "    \n",
    "    df_recommendations_popular = create_recommendations_from_one_model(pop_model, data.users_for_prediction_popular)\n",
    "    df_recommendations = pd.concat([df_recommendations, df_recommendations_popular], axis = 0)\n",
    "    metrics = compute_metrics(train_interactions, test_interactions, df_recommendations)\n",
    "    print(\"The time for evaluate_model is: \" + time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time)))\n",
    "    return metrics\n",
    "\n",
    "def calculate_test_loss(model, test_loader, loss_function):\n",
    "    start_time = time.time()\n",
    "    loss_accum = 0\n",
    "    model.eval()\n",
    "    for i, (user, item, label) in enumerate(test_loader):\n",
    "        user = user.to(device)\n",
    "        item = item.to(device)\n",
    "        label = label.to(device)\n",
    "        prediction = model(user, item).squeeze()\n",
    "        loss = loss_function(prediction, label)\n",
    "        loss_accum += loss\n",
    "    print(\"The time for test is: \" + time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time)))\n",
    "    return loss_accum\n",
    "\n",
    "def train_model(model, train_loader, test_loader, loss_function, optimizer, scheduler, num_epochs):\n",
    "    loss_history_train = []\n",
    "    loss_history_test = []\n",
    "    test_metrics_history = []\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"epoch: {epoch}\")\n",
    "        start_time = time.time()\n",
    "        loss_accum = 0\n",
    "        train_loader = data.get_train_instance() # refresh negative samples\n",
    "        model.train() # Enter train mode\n",
    "        for (user, item, label) in tqdm(train_loader):\n",
    "            user = user.to(device)\n",
    "            item = item.to(device)\n",
    "            label = label.to(device)  \n",
    "            prediction = model(user, item).squeeze()\n",
    "            loss = loss_function(prediction, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_accum += loss\n",
    "            \n",
    "        loss_accum = loss_accum\n",
    "        loss_history_train.append(loss_accum)\n",
    "        test_loss = calculate_test_loss(model, test_loader, loss_function)\n",
    "        loss_history_test.append(test_loss)\n",
    "        metrics = evaluate_model(model, batch_size=64)\n",
    "        test_metrics_history.append(metrics)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        print(f\"train_loss: {loss_accum} \\ntest_loss: {test_loss} \\nmetrics_on_test_set: \\n{metrics}\")\n",
    "        print(\"The time elapse of epoch {:03d}\".format(epoch) + \" is: \" + time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time)))\n",
    "        print(\"\\n\")\n",
    "    return loss_history_train, loss_history_test, test_metrics_history\n",
    "loss_history_train, loss_history_test, test_metrics_history = train_model(model, train_loader, test_loader, loss_function, optimizer, scheduler, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5453b2e3",
   "metadata": {},
   "source": [
    "## NeuMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5860dc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T23:41:00.481590Z",
     "start_time": "2022-07-03T23:41:00.472820Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, factor_num_mf, factor_num_mlp):\n",
    "        super(NeuMF, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.factor_num_mf = factor_num_mf\n",
    "        self.factor_num_mlp =  factor_num_mlp\n",
    "        self.layers = [2*factor_num_mlp, 32, 16]\n",
    "        self.dropout = 0.2\n",
    "        self.reg_1=0.0003\n",
    "        self.reg_2=0.0003\n",
    "        self.embedding_user_mlp = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_mlp)\n",
    "        self.embedding_item_mlp = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_mlp)\n",
    "\n",
    "        self.embedding_user_mf = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_mf)\n",
    "        self.embedding_item_mf = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_mf)\n",
    "\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        for idx, (in_size, out_size) in enumerate(zip(self.layers[:-1], self.layers[1:])):\n",
    "            self.fc_layers.append(nn.Dropout(p=self.dropout))\n",
    "            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
    "            self.fc_layers.append(nn.ReLU())\n",
    "\n",
    "        self.affine_output = nn.Linear(in_features=self.layers[-1] + self.factor_num_mf, out_features=1)\n",
    "        self.logistic = nn.Sigmoid()\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        nn.init.normal_(self.embedding_user_mlp.weight, std=0.01)\n",
    "        nn.init.normal_(self.embedding_item_mlp.weight, std=0.01)\n",
    "        nn.init.normal_(self.embedding_user_mf.weight, std=0.01)\n",
    "        nn.init.normal_(self.embedding_item_mf.weight, std=0.01)\n",
    "        \n",
    "        for m in self.fc_layers:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                \n",
    "        nn.init.xavier_uniform_(self.affine_output.weight)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embedding_mlp = self.embedding_user_mlp(user_indices)\n",
    "        item_embedding_mlp = self.embedding_item_mlp(item_indices)\n",
    "\n",
    "        user_embedding_mf = self.embedding_user_mf(user_indices)\n",
    "        item_embedding_mf = self.embedding_item_mf(item_indices)\n",
    "\n",
    "        mlp_vector = torch.cat([user_embedding_mlp, item_embedding_mlp], dim=-1)  # the concat latent vector\n",
    "        mf_vector = torch.mul(user_embedding_mf, item_embedding_mf)\n",
    "\n",
    "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
    "            mlp_vector = self.fc_layers[idx](mlp_vector)\n",
    "\n",
    "        vector = torch.cat([mlp_vector, mf_vector], dim=-1)\n",
    "        logits = self.affine_output(vector)\n",
    "        rating = self.logistic(logits)\n",
    "        return rating.squeeze()\n",
    "    \n",
    "    def recommend(self, users, batch_size, n = config.num_recommendations, show_progress_bar = False):\n",
    "        recs = list()\n",
    "        user_count = len(users)\n",
    "        \n",
    "        for start in tqdm(\n",
    "            range(0, user_count, batch_size),\n",
    "            desc=\"predict from model, users:\",\n",
    "            disable = not show_progress_bar\n",
    "        ):\n",
    "            end = start + batch_size\n",
    "            if end > user_count:\n",
    "                end = user_count\n",
    "                \n",
    "            user_indices = torch.tensor(users[start:end]).to(device).repeat_interleave(self.num_items)\n",
    "            item_indices = torch.tensor(range(self.num_items)).to(device).repeat((end-start))\n",
    "            predict = self.forward(user_indices, item_indices).view((end-start), -1)\n",
    "            predict = predict.topk(n)[1].detach().cpu()\n",
    "            \n",
    "            for rec in predict:\n",
    "                recs.append(rec.tolist())\n",
    "            \n",
    "        return recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e407002b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T23:41:01.622308Z",
     "start_time": "2022-07-03T23:41:00.483013Z"
    }
   },
   "outputs": [],
   "source": [
    "model = NeuMF(len(data.user2id), len(data.item2id), factor_num_mf=32, factor_num_mlp=32)\n",
    "model = model.to(device)\n",
    "model.init_weight()\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), 0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bd72aac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T23:41:01.626474Z",
     "start_time": "2022-07-03T23:41:01.623961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuMF(\n",
      "  (embedding_user_mlp): Embedding(281156, 32)\n",
      "  (embedding_item_mlp): Embedding(8250, 32)\n",
      "  (embedding_user_mf): Embedding(281156, 32)\n",
      "  (embedding_item_mf): Embedding(8250, 32)\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (affine_output): Linear(in_features=48, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3665df9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-03T23:28:36.899134Z",
     "start_time": "2022-07-03T21:57:01.802846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 76329/76329 [14:35<00:00, 87.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 30\n",
      "The time for evaluate_model is: 00: 00: 48\n",
      "train_loss: 97574.6015625 \n",
      "test_loss: 5609.150390625\n",
      "metrics_on_test_set: \n",
      "{'Precision@1': 0.07489781772115592, 'Recall@1': 0.04826541257304038, 'Precision@3': 0.07614870409764879, 'Recall@3': 0.13749998065509733, 'Precision@10': 0.037898271864617444, 'Recall@10': 0.21208168366755134, 'MAP@10': 0.1032774702108003, 'Novelty@10': 3.437181827618628}\n",
      "The time elapse of epoch 001 is: 00: 15: 55\n",
      "\n",
      "\n",
      "epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 76329/76329 [14:55<00:00, 85.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 28\n",
      "The time for evaluate_model is: 00: 00: 45\n",
      "train_loss: 67007.8671875 \n",
      "test_loss: 5276.1259765625\n",
      "metrics_on_test_set: \n",
      "{'Precision@1': 0.07835767382938547, 'Recall@1': 0.05012668924625102, 'Precision@3': 0.07483806200253362, 'Recall@3': 0.13515856162103904, 'Precision@10': 0.0378600282046992, 'Recall@10': 0.2115794181373215, 'MAP@10': 0.1035088512445733, 'Novelty@10': 3.4377721751132544}\n",
      "The time elapse of epoch 002 is: 00: 16: 09\n",
      "\n",
      "\n",
      "epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 76329/76329 [13:53<00:00, 91.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 31\n",
      "The time for evaluate_model is: 00: 00: 42\n",
      "train_loss: 48636.90625 \n",
      "test_loss: 5246.63720703125\n",
      "metrics_on_test_set: \n",
      "{'Precision@1': 0.07835767382938547, 'Recall@1': 0.05012668924625102, 'Precision@3': 0.07483607014524621, 'Recall@3': 0.13515407994214237, 'Precision@10': 0.039313884838779074, 'Recall@10': 0.2200835753291695, 'MAP@10': 0.10444494617552601, 'Novelty@10': 3.438662809413435}\n",
      "The time elapse of epoch 003 is: 00: 15: 07\n",
      "\n",
      "\n",
      "epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 76329/76329 [13:49<00:00, 91.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 28\n",
      "The time for evaluate_model is: 00: 00: 43\n",
      "train_loss: 38386.3828125 \n",
      "test_loss: 5042.4501953125\n",
      "metrics_on_test_set: \n",
      "{'Precision@1': 0.07495757343977819, 'Recall@1': 0.048289717758895144, 'Precision@3': 0.07333221789325237, 'Recall@3': 0.13319189319599126, 'Precision@10': 0.037861223319071644, 'Recall@10': 0.21158648923069182, 'MAP@10': 0.10285298682520227, 'Novelty@10': 3.437772729413715}\n",
      "The time elapse of epoch 004 is: 00: 15: 01\n",
      "\n",
      "\n",
      "epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 76329/76329 [13:39<00:00, 93.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 27\n",
      "The time for evaluate_model is: 00: 00: 43\n",
      "train_loss: 32451.796875 \n",
      "test_loss: 5079.1591796875\n",
      "metrics_on_test_set: \n",
      "{'Precision@1': 0.07502330473026268, 'Recall@1': 0.0482829368538078, 'Precision@3': 0.07333221789325237, 'Recall@3': 0.13319189319599126, 'Precision@10': 0.039313884838779074, 'Recall@10': 0.2200835753291695, 'MAP@10': 0.10378522319267298, 'Novelty@10': 3.4386624549522558}\n",
      "The time elapse of epoch 005 is: 00: 14: 51\n",
      "\n",
      "\n",
      "epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 76329/76329 [13:57<00:00, 91.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 30\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 6.00 GiB total capacity; 5.25 GiB already allocated; 0 bytes free; 5.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 72>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_history_train, loss_history_test, test_metrics_history\n\u001b[0;32m---> 72\u001b[0m loss_history_train, loss_history_test, test_metrics_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, loss_function, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     59\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m calculate_test_loss(model, test_loader, loss_function)\n\u001b[1;32m     60\u001b[0m loss_history_test\u001b[38;5;241m.\u001b[39mappend(test_loss)\n\u001b[0;32m---> 61\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m test_metrics_history\u001b[38;5;241m.\u001b[39mappend(metrics)\n\u001b[1;32m     64\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, batch_size)\u001b[0m\n\u001b[1;32m      2\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 4\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecommend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser2id\u001b[49m\u001b[43m[\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43musers_for_prediction_model\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m preds \u001b[38;5;241m=\u001b[39m [[data\u001b[38;5;241m.\u001b[39mitem2id_reverse[item] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m str_] \u001b[38;5;28;01mfor\u001b[39;00m str_ \u001b[38;5;129;01min\u001b[39;00m preds]\n\u001b[1;32m      7\u001b[0m df_recommendations \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m: data\u001b[38;5;241m.\u001b[39musers_for_prediction_model})\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mNeuMF.recommend\u001b[0;34m(self, users, batch_size, n, show_progress_bar)\u001b[0m\n\u001b[1;32m     75\u001b[0m user_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(users[start:end])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mrepeat_interleave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_items)\n\u001b[1;32m     76\u001b[0m item_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_items))\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mrepeat((end\u001b[38;5;241m-\u001b[39mstart))\n\u001b[0;32m---> 77\u001b[0m predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview((end\u001b[38;5;241m-\u001b[39mstart), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     78\u001b[0m predict \u001b[38;5;241m=\u001b[39m predict\u001b[38;5;241m.\u001b[39mtopk(n)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rec \u001b[38;5;129;01min\u001b[39;00m predict:\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mNeuMF.forward\u001b[0;34m(self, user_indices, item_indices)\u001b[0m\n\u001b[1;32m     52\u001b[0m mf_vector \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmul(user_embedding_mf, item_embedding_mf)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_layers))):\n\u001b[0;32m---> 55\u001b[0m     mlp_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp_vector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m vector \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([mlp_vector, mf_vector], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     58\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maffine_output(vector)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py:98\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 6.00 GiB total capacity; 5.25 GiB already allocated; 0 bytes free; 5.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, batch_size):\n",
    "    start_time = time.time()\n",
    "    model.eval()\n",
    "    preds = model.recommend([data.user2id[user_id] for user_id in data.users_for_prediction_model], batch_size = batch_size)\n",
    "    preds = [[data.item2id_reverse[item] for item in str_] for str_ in preds]\n",
    "    \n",
    "    df_recommendations = pd.DataFrame({'user_id': data.users_for_prediction_model})\n",
    "    df_recommendations['item_id'] = preds\n",
    "    df_recommendations = df_recommendations.explode('item_id')\n",
    "    df_recommendations['rank'] = df_recommendations.groupby('user_id').cumcount() + 1\n",
    "    \n",
    "    df_recommendations_popular = create_recommendations_from_one_model(pop_model, data.users_for_prediction_popular)\n",
    "    df_recommendations = pd.concat([df_recommendations, df_recommendations_popular], axis = 0)\n",
    "    metrics = compute_metrics(train_interactions, test_interactions, df_recommendations)\n",
    "    print(\"The time for evaluate_model is: \" + time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time)))\n",
    "    return metrics\n",
    "\n",
    "def calculate_test_loss(model, test_loader, loss_function):\n",
    "    start_time = time.time()\n",
    "    loss_accum = 0\n",
    "    model.eval()\n",
    "    for i, (user, item, label) in enumerate(test_loader):\n",
    "        user = user.to(device)\n",
    "        item = item.to(device)\n",
    "        label = label.to(device)\n",
    "        prediction = model(user, item).squeeze()\n",
    "        loss = loss_function(prediction, label)\n",
    "        loss_accum += loss\n",
    "    print(\"The time for test is: \" + time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time)))\n",
    "    return loss_accum\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, loss_function, optimizer, scheduler, num_epochs):\n",
    "    loss_history_train = []\n",
    "    loss_history_test = []\n",
    "    test_metrics_history = []\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"epoch: {epoch}\")\n",
    "        start_time = time.time()\n",
    "        loss_accum = 0\n",
    "        #train_loader = data.get_train_instance() # refresh negative samples\n",
    "        model.train() # Enter train mode\n",
    "        for (user, item, label) in tqdm(train_loader):\n",
    "            user = user.to(device)\n",
    "            item = item.to(device)\n",
    "            label = label.to(device)\n",
    "            prediction = model(user, item).squeeze()\n",
    "            loss = loss_function(prediction, label)\n",
    "            loss += model.reg_1 * (model.embedding_item_mf.weight.norm(p=1) + model.embedding_user_mf.weight.norm(p=1))\n",
    "            loss += model.reg_2 * (model.embedding_item_mlp.weight.norm(p=1) + model.embedding_user_mlp.weight.norm(p=1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_accum += loss\n",
    "            \n",
    "        loss_accum = loss_accum\n",
    "        loss_history_train.append(loss_accum)\n",
    "        test_loss = calculate_test_loss(model, test_loader, loss_function)\n",
    "        loss_history_test.append(test_loss)\n",
    "        metrics = evaluate_model(model, batch_size=64)\n",
    "        test_metrics_history.append(metrics)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        print(f\"train_loss: {loss_accum} \\ntest_loss: {test_loss}\\nmetrics_on_test_set: \\n{metrics}\")\n",
    "        print(\"The time elapse of epoch {:03d}\".format(epoch) + \" is: \" + time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time)))\n",
    "        print(\"\\n\")\n",
    "    return loss_history_train, loss_history_test, test_metrics_history\n",
    "loss_history_train, loss_history_test, test_metrics_history = train_model(model, train_loader, test_loader, loss_function, optimizer, scheduler, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34d49cbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T01:21:14.215900Z",
     "start_time": "2022-07-03T23:41:01.628056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 76329/76329 [14:44<00:00, 86.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 30\n",
      "The time for evaluate_model is: 00: 00: 57\n",
      "train_loss: 98303.6640625 \n",
      "test_loss: 5764.9697265625\n",
      "metrics_on_test_set: \n",
      "{'Precision@1': 0.07489781772115592, 'Recall@1': 0.04826541257304038, 'Precision@3': 0.07368278477583638, 'Recall@3': 0.13329429716189722, 'Precision@10': 0.037126825537203895, 'Recall@10': 0.2084377180345538, 'MAP@10': 0.10152188390108692, 'Novelty@10': 3.4581507048027404}\n",
      "The time elapse of epoch 001 is: 00: 17: 23\n",
      "\n",
      "\n",
      "epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 76329/76329 [14:52<00:00, 85.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 31\n",
      "The time for evaluate_model is: 00: 00: 52\n",
      "train_loss: 66023.2578125 \n",
      "test_loss: 5376.5205078125\n",
      "metrics_on_test_set: \n",
      "{'Precision@1': 0.07490379329301815, 'Recall@1': 0.04826740443032779, 'Precision@3': 0.0732027471695708, 'Recall@3': 0.13344516357914507, 'Precision@10': 0.039313884838779074, 'Recall@10': 0.2200835753291695, 'MAP@10': 0.10352089555124756, 'Novelty@10': 3.4386627717716025}\n",
      "The time elapse of epoch 002 is: 00: 17: 23\n",
      "\n",
      "\n",
      "epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 76329/76329 [15:22<00:00, 82.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 31\n",
      "The time for evaluate_model is: 00: 00: 49\n",
      "train_loss: 47836.6015625 \n",
      "test_loss: 5329.32470703125\n",
      "metrics_on_test_set: \n",
      "{'Precision@1': 0.07489781772115592, 'Recall@1': 0.04826541257304038, 'Precision@3': 0.07368278477583638, 'Recall@3': 0.13329429716189722, 'Precision@10': 0.03789946697898989, 'Recall@10': 0.21207736560550328, 'MAP@10': 0.10217672885379782, 'Novelty@10': 3.43718241915476}\n",
      "The time elapse of epoch 003 is: 00: 17: 51\n",
      "\n",
      "\n",
      "epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 76329/76329 [13:53<00:00, 91.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 28\n",
      "The time for evaluate_model is: 00: 00: 46\n",
      "train_loss: 37870.796875 \n",
      "test_loss: 5153.16015625\n",
      "metrics_on_test_set: \n",
      "{'Precision@1': 0.07941535004899969, 'Recall@1': 0.050229444716384276, 'Precision@3': 0.07616463895594809, 'Recall@3': 0.1375204993719853, 'Precision@10': 0.0392804216363506, 'Recall@10': 0.21989703968932534, 'MAP@10': 0.10545085630115404, 'Novelty@10': 3.4386245951711336}\n",
      "The time elapse of epoch 004 is: 00: 16: 11\n",
      "\n",
      "\n",
      "epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 76329/76329 [13:46<00:00, 92.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 28\n",
      "The time for evaluate_model is: 00: 00: 48\n",
      "train_loss: 32085.591796875 \n",
      "test_loss: 5197.23583984375\n",
      "metrics_on_test_set: \n",
      "{'Precision@1': 0.07489781772115592, 'Recall@1': 0.04826541257304038, 'Precision@3': 0.07616463895594809, 'Recall@3': 0.1375204993719853, 'Precision@10': 0.0378600282046992, 'Recall@10': 0.2115794181373215, 'MAP@10': 0.1032210786076748, 'Novelty@10': 3.4377721751132544}\n",
      "The time elapse of epoch 005 is: 00: 16: 04\n",
      "\n",
      "\n",
      "epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 76329/76329 [13:47<00:00, 92.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for test is: 00: 00: 27\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 6.00 GiB total capacity; 5.25 GiB already allocated; 0 bytes free; 5.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 72>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_history_train, loss_history_test, test_metrics_history\n\u001b[0;32m---> 72\u001b[0m loss_history_train, loss_history_test, test_metrics_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, loss_function, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     59\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m calculate_test_loss(model, test_loader, loss_function)\n\u001b[1;32m     60\u001b[0m loss_history_test\u001b[38;5;241m.\u001b[39mappend(test_loss)\n\u001b[0;32m---> 61\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m test_metrics_history\u001b[38;5;241m.\u001b[39mappend(metrics)\n\u001b[1;32m     64\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, batch_size)\u001b[0m\n\u001b[1;32m      2\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 4\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecommend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser2id\u001b[49m\u001b[43m[\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43musers_for_prediction_model\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m preds \u001b[38;5;241m=\u001b[39m [[data\u001b[38;5;241m.\u001b[39mitem2id_reverse[item] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m str_] \u001b[38;5;28;01mfor\u001b[39;00m str_ \u001b[38;5;129;01min\u001b[39;00m preds]\n\u001b[1;32m      7\u001b[0m df_recommendations \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m: data\u001b[38;5;241m.\u001b[39musers_for_prediction_model})\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mNeuMF.recommend\u001b[0;34m(self, users, batch_size, n, show_progress_bar)\u001b[0m\n\u001b[1;32m     75\u001b[0m user_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(users[start:end])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mrepeat_interleave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_items)\n\u001b[1;32m     76\u001b[0m item_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_items))\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mrepeat((end\u001b[38;5;241m-\u001b[39mstart))\n\u001b[0;32m---> 77\u001b[0m predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview((end\u001b[38;5;241m-\u001b[39mstart), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     78\u001b[0m predict \u001b[38;5;241m=\u001b[39m predict\u001b[38;5;241m.\u001b[39mtopk(n)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rec \u001b[38;5;129;01min\u001b[39;00m predict:\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mNeuMF.forward\u001b[0;34m(self, user_indices, item_indices)\u001b[0m\n\u001b[1;32m     52\u001b[0m mf_vector \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmul(user_embedding_mf, item_embedding_mf)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_layers))):\n\u001b[0;32m---> 55\u001b[0m     mlp_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp_vector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m vector \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([mlp_vector, mf_vector], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     58\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maffine_output(vector)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py:98\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 6.00 GiB total capacity; 5.25 GiB already allocated; 0 bytes free; 5.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, batch_size):\n",
    "    start_time = time.time()\n",
    "    model.eval()\n",
    "    preds = model.recommend([data.user2id[user_id] for user_id in data.users_for_prediction_model], batch_size = batch_size)\n",
    "    preds = [[data.item2id_reverse[item] for item in str_] for str_ in preds]\n",
    "    \n",
    "    df_recommendations = pd.DataFrame({'user_id': data.users_for_prediction_model})\n",
    "    df_recommendations['item_id'] = preds\n",
    "    df_recommendations = df_recommendations.explode('item_id')\n",
    "    df_recommendations['rank'] = df_recommendations.groupby('user_id').cumcount() + 1\n",
    "    \n",
    "    df_recommendations_popular = create_recommendations_from_one_model(pop_model, data.users_for_prediction_popular)\n",
    "    df_recommendations = pd.concat([df_recommendations, df_recommendations_popular], axis = 0)\n",
    "    metrics = compute_metrics(train_interactions, test_interactions, df_recommendations)\n",
    "    print(\"The time for evaluate_model is: \" + time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time)))\n",
    "    return metrics\n",
    "\n",
    "def calculate_test_loss(model, test_loader, loss_function):\n",
    "    start_time = time.time()\n",
    "    loss_accum = 0\n",
    "    model.eval()\n",
    "    for i, (user, item, label) in enumerate(test_loader):\n",
    "        user = user.to(device)\n",
    "        item = item.to(device)\n",
    "        label = label.to(device)\n",
    "        prediction = model(user, item).squeeze()\n",
    "        loss = loss_function(prediction, label)\n",
    "        loss_accum += loss\n",
    "    print(\"The time for test is: \" + time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time)))\n",
    "    return loss_accum\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, loss_function, optimizer, scheduler, num_epochs):\n",
    "    loss_history_train = []\n",
    "    loss_history_test = []\n",
    "    test_metrics_history = []\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"epoch: {epoch}\")\n",
    "        start_time = time.time()\n",
    "        loss_accum = 0\n",
    "        train_loader = data.get_train_instance() # refresh negative samples\n",
    "        model.train() # Enter train mode\n",
    "        for (user, item, label) in tqdm(train_loader):\n",
    "            user = user.to(device)\n",
    "            item = item.to(device)\n",
    "            label = label.to(device)\n",
    "            prediction = model(user, item).squeeze()\n",
    "            loss = loss_function(prediction, label)\n",
    "            loss += model.reg_1 * (model.embedding_item_mf.weight.norm(p=1) + model.embedding_user_mf.weight.norm(p=1))\n",
    "            loss += model.reg_2 * (model.embedding_item_mlp.weight.norm(p=1) + model.embedding_user_mlp.weight.norm(p=1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_accum += loss\n",
    "            \n",
    "        loss_accum = loss_accum\n",
    "        loss_history_train.append(loss_accum)\n",
    "        test_loss = calculate_test_loss(model, test_loader, loss_function)\n",
    "        loss_history_test.append(test_loss)\n",
    "        metrics = evaluate_model(model, batch_size=64)\n",
    "        test_metrics_history.append(metrics)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        print(f\"train_loss: {loss_accum} \\ntest_loss: {test_loss}\\nmetrics_on_test_set: \\n{metrics}\")\n",
    "        print(\"The time elapse of epoch {:03d}\".format(epoch) + \" is: \" + time.strftime(\"%H: %M: %S\", time.gmtime(time.time() - start_time)))\n",
    "        print(\"\\n\")\n",
    "    return loss_history_train, loss_history_test, test_metrics_history\n",
    "loss_history_train, loss_history_test, test_metrics_history = train_model(model, train_loader, test_loader, loss_function, optimizer, scheduler, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c33a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
